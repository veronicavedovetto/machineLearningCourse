---
title: "final_assignment"
author: "Veronica Vedovetto"
date: "06 marzo 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
#load("./machine_learning.RData")
```

## Introduction

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.
In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.
They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset).


## Execution
First I downloaded the training and test datasets and read them.
In the training I found that there were some strings that indicated missing values, so I read them properly.
```{r warning=FALSE, message = FALSE}

training <- read_csv("./pml-training.csv", na = c("", "NA","#DIV/0!"))
testing <- read_csv("./pml-testing.csv")

```

Then I inspected the training set and found that there were a lot of features with a lot of missing values

```{r warning=FALSE, message = FALSE}
#Counting how many missings in each group "user-classe".
missing_df <- training[,-1] %>% 
  group_by(user_name,classe) %>% 
  summarise_all(function(x){sum(is.na(x))}) %>% 
  ungroup()

#Counting how many rows in each group "user-classe".
info_base <- training[,-1] %>% 
  group_by(user_name,classe) %>% 
  summarise(tot_righe = n()) %>% 
  ungroup() 

#Calculating % of missing.
missing_df <- left_join(missing_df,info_base)
missing_df <- missing_df %>% 
  mutate_at(vars(-user_name,-classe), function(x,y = missing_df$tot_righe){x/y})

#To see what's the minimum % of missings.
min_values <- missing_df %>% 
  select(-user_name,-classe) %>% 
  summarise_all(min)

head(min_values[,c(1:5,20:22)])

```

I decided to use only features without missings, so I created a new training set containing only those features.
I splitted this new training set in two datasets: true training set and a test set.
The original test set didn't cointain the classe variable so it cannot be used as a standard test set.

```{r warning=FALSE, message = FALSE}

training2 <- select_if(training, function(x){sum(is.na(x)) == 0})

library(caret)
set.seed(12345)
Intrain <- createDataPartition(training2$classe, p = 0.7, list = FALSE) 
training_set <- training2[Intrain,]
testing_set <- training2[-Intrain,]



```

I decided to try 2 different algorithms "Random Forest" and "Linear Discriminant analysis". I tried also to preprocess the training set using PCA (principal component analisys).

As we can see the last model has the better performance both in the training and test set, so I used it to predict the 20 records of the original testing set. The discriminant analysis, instead, performed badly.
The OOB of the third model is aproximately around 0.64%.
If the purpose is to predict the class of the exercise without giving specific indications on what is going wrong, then the random forest will be a possible choice, even though it can produce an order of importance of the features, so we can understand which parameter of accellerators could be an important feature that discriminates how well is done the eexercise.

```{r }
#Keep only feature about data from accellerators (already left out the ones with too many missing values)
#Try to see what's the first output of pca 
pca_1 <- preProcess(training_set[, -c(1:7,60)], method = "pca" )
pca_1
#Corrected with a lower number of components
pca_2 <- preProcess(training_set[, -c(1:7,60)], method = "pca" ,thresh = 0.90)
pca_2

#Apply preprocessing to training set
feature_pca <- predict(pca_2, training_set)
#Keep only classe and features (pca components)
feature_pca_rid <- feature_pca[,-c(1:7)]

#Set 5-fold crossvalidation as the method for training the algorithm
rf_control <- trainControl(method = "cv", number = 5, classProbs = TRUE)
```

First model: Random Forest with pca preprocessing
```{r }
mod1 <- train(classe~., data= feature_pca_rid, method = "rf", trControl = rf_control)
mod1
mod1$finalModel
```

Applying the estimated model to the testing set
```{r}
testing_set_PCA <- predict(pca_2, testing_set)
pred_mod1 <- predict(mod1, testing_set_PCA)

confusionMatrix(table(pred_mod1, testing_set_PCA$classe))
```

Second model: Linear discriminant analysis with pca preprocessing
```{r}
mod2 <- train(classe~., data = feature_pca_rid, method = "lda", trControl = rf_control)
mod2
#mod2$finalModel
pred_mod2 <- predict(mod2, testing_set_PCA)

confusionMatrix(table(pred_mod2, testing_set_PCA$classe))
```

Third model: Random Forest with original features
```{r}
training_set_rid <- training_set[, -c(1:7)]
mod3 <- train(classe~., data= training_set_rid, method = "rf", trControl = rf_control)
mod3
mod3$finalModel


pred_mod3 <- predict(mod3, testing_set)

confusionMatrix(table(pred_mod3, testing_set$classe))
```
Importance of variables used in the model:
```{r}
varImp(mod3)

```

